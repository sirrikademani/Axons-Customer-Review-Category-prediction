{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2ea97b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing pre-requisite Libraries.\n",
    "# ------------------------------------------------------------------------------------------------------------------------------\n",
    "import pandas as pd                                                     # Pandas offers data structures and operations for manipulating numerical tables and time series.\n",
    "import numpy as np                                                      # NumPy offers support for large, multi-dimensional arrays and matrices, along with a large collection \n",
    "                                                                        # of high-level mathematical functions to operate on these arrays.\n",
    "import pickle                                                           # Using Pickle to convert a Python object into a byte stream to store it in a file/database, \n",
    "                                                                        # maintain program state across sessions, making it easier to work with in deployments. \n",
    "from datetime import datetime\n",
    "\n",
    "# Importing Sklearn Models and the required Libraries for ML operations.\n",
    "# ------------------------------------------------------------------------------------------------------------------------------\n",
    "from sklearn.linear_model import LogisticRegression                     # The logistic model is a statistical model that models the probability of one event taking place \n",
    "                                                                        # by having the log-odds for the event be a linear combination of one or more independent variables.                      \n",
    "from sklearn.neighbors import KNeighborsClassifier                      # K-nearest neighbors is a supervised learning algorithm used for classification and regression tasks.\n",
    "from sklearn import tree                                                # Importing the tree module from scikit-learn (sklearn) library for decision tree-based machine learning models.\n",
    "from sklearn.tree import DecisionTreeClassifier                         # DecisionTreeClassifier is used for creating decision tree models, which are a type of supervised learning algorithm used for both classification and regression tasks.\n",
    "from sklearn.ensemble import RandomForestClassifier                     # Random Forest Classifier is an ensemble learning method that combines multiple decision trees to create a more robust and accurate model.\n",
    "from sklearn.svm import SVC                                             # The SVC class provides an implementation of the SVM algorithm for classification tasks. It can handle both linear and non-linear data by finding the optimal hyperplane or kernel function to separate classes.\n",
    "from imblearn.over_sampling import SMOTE                                # The SMOTE function is used to generate synthetic examples of the minority class in imbalanced datasets. It creates synthetic examples by interpolating between instances of the minority class, effectively increasing the number of minority class samples in the dataset.\n",
    "\n",
    "# Importing the metrics module for evaluating model performance.\n",
    "# ------------------------------------------------------------------------------------------------------------------------------    \n",
    "from sklearn.metrics import make_scorer, precision_score, recall_score, accuracy_score, f1_score         \n",
    "                                                                        # Create a custom scoring function using the make_scorer function for model evaluation during cross-validation.\n",
    "                                                                        # Precision, recall, accuracy, and F1-score are metrics used for evaluating classification models.\n",
    "                                                                        # These metrics provide insights into different aspects of model performance, such as the ability to correctly classify positive and negative samples, overall accuracy, and the trade-off between precision and recall.\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_absolute_error, r2_score, mean_squared_error\n",
    "                                                                        # The mean_absolute_percentage_error, mean_absolute_error, r2_score, and mean_squared_error functions are commonly used metrics for evaluating regression models.\n",
    "import statsmodels.api as sm                                            # The statsmodels.api module is a Python library for conducting statistical analysis, including linear regression, logistic regression, time series analysis, and more.\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV, cross_val_score, KFold\n",
    "                                                                        # The train_test_split function is commonly used to split the data into training and testing sets for model evaluation and validation.\n",
    "                                                                        # The cross_val_score function is used for cross-validation, which is a technique for assessing the performance of a model by training and evaluating it on multiple subsets of data.\n",
    "                                                                        # The RandomizedSearchCV function is used for hyperparameter tuning using randomized search, which is a technique for finding optimal hyperparameter values for a model by sampling from a distribution of possible values.\n",
    "                                                                        # The KFold function is used to create K-fold cross-validation splits, where K is the number of folds. It can be used in conjunction with cross_val_score for performing K-fold cross-validation.\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix     # The confusion_matrix function is used to compute the confusion matrix, which is a table that describes the performance of a classification model by showing the counts of true positive, true negative, false positive, and false negative predictions.\n",
    "                                                                        # The plot_confusion_matrix function is used to plot the confusion matrix for visualization purposes, making it easier to interpret and analyze the performance of a classification model.\n",
    "from sklearn import metrics                                             # Importing error metrics here.\n",
    "\n",
    "# Importing libraries for NLP operations.\n",
    "# ------------------------------------------------------------------------------------------------------------------------------\n",
    "import spacy                                                            # The spacy library is a popular tool for natural language processing (NLP) in Python. It provides various NLP functionalities, such as tokenization, part-of-speech tagging, named entity recognition, dependency parsing, and more.\n",
    "import gensim.downloader as api                                         # The api module in Gensim allows for easy access to pre-trained word embeddings, which are word representation vectors learned from large corpora of text. These pre-trained word embeddings can be used as features in natural language processing (NLP) and text mining tasks, such as word similarity, document classification, and text generation.\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "wv = KeyedVectors.load_word2vec_format('word2vec-google-news-300.bin', binary=True)                               \n",
    "                                                                        # The 'word2vec-google-news-300' pre-trained word embeddings are trained on a large corpus of Google News articles and consist of 300-dimensional word vectors. These word vectors can be used to represent words in a meaningful way for various NLP tasks, such as word similarity, document classification, and text generation.\n",
    "nlp = spacy.load(\"spacy_model_updated_v2\")                              # loads the pre-trained English language model \"en_core_web_lg\" from the Spacy library. \n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------------\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")                                         # Warnings have been turned off, after carefuly examining each of them\n",
    "pd.set_option('display.max_columns', None)                              # To print the above output in a wider format. This attribute is used to set the no. of columns \n",
    "pd.set_option('display.max_rows', None)                                 # To print the above output in a wider format. This attribute is used to set the no. of columns \n",
    "pd.options.display.float_format = '{:.2f}'.format                       # To get rid of scientific notations used to disply large numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92ebdfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Preprosessing the free-text data, by lemmatizing, removing stop words, and punctuations. This function returns a processed\n",
    "# free-text data (one line) joined by space. review (str): Input text to be normalized, lowercase (bool): Flag indicating whether to convert the text to lowercase. remove_stopwords (bool): Flag indicating whether to remove stopwords.         punctuations_rm (bool): Flag indicating whether to remove punctuations\n",
    "\n",
    "def normalize(review, lowercase, remove_stopwords, punctuations_rm):\n",
    "    if lowercase:\n",
    "        review = review.lower()                                        # Converting the text to lowercase if the `lowercase` flag is set\n",
    "    doc = nlp(review)\n",
    "    lemmatized = list()\n",
    "    for token in doc:\n",
    "          if remove_stopwords and not token.is_stop:                   # Checking if the token is a stopword and the `remove_stopwords` flag is set\n",
    "            if punctuations_rm:\n",
    "                if not token.is_punct:                                 # Checking if the token is a punctuation and the `punctuations_rm` flag is set\n",
    "                    lemmatized.append(token.lemma_)                    # Lemmatizing the token and appending to the list\n",
    "            else: \n",
    "                lemmatized.append(token.lemma_)\n",
    "    return \" \".join(lemmatized)                                        # Joining the lemmatized tokens with space and returning as normalized text\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Function to generate word embeddings using pre-trained word vectors\n",
    "\n",
    "def generate_word_embeddings(sent):\n",
    "    words = sent.split()                                               # Splitting the input sentence into words\n",
    "    word_vectors = [wv[word] for word in words if word in wv]          # Extracting word vectors for words present in the pre-trained word vectors\n",
    "    if word_vectors:\n",
    "        return np.mean(word_vectors, axis=0)                           # Calculating the average word vector\n",
    "    else:\n",
    "        return np.zeros((300,))                                        # Returning a zero vector if no word vectors are found for the input sentence\n",
    "    \n",
    "    \n",
    "# ------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Function to create a new vector that keeps a check on the type of ner tags used in the sentences and mark it as 1 on encountering the respective ner tag, to further feed this as a feature to the model.\n",
    "\n",
    "def generate_ner_tags(doc):\n",
    "    ner_dict = dict.fromkeys(labels, 0)                                # Creating a dictionary to store NER labels and initializing all values to 0\n",
    "    # Using a dictionary comprehension\n",
    "    ner_dict = {labels: 0 for labels in labels}\n",
    "    doc = nlp(doc)\n",
    "    for tokens in doc.ents:\n",
    "        if tokens.label_ in ner_dict:                                  # Checking if the NER label is present in the dictionary\n",
    "            ner_dict[tokens.label_]=1                                  # Setting the value to 1 if the NER label is found\n",
    "    return(list(ner_dict.values()))                                    # Converting the dictionary values to a list and returning\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Function to pre-process the given sentence and return a numpy array of features\n",
    "\n",
    "    \n",
    "def testing(sent):\n",
    "    # Calling the normalize function to preprocess the sentence by removing stopwords and punctuations, and making it lowercase.\n",
    "    sent = normalize(sent, lowercase=True, remove_stopwords=True, punctuations_rm=True)\n",
    "    # Counting the number of tokens in the processed sentence using the nlp object, to pass it as a feature to the model.\n",
    "    sent_length = len(nlp(sent))\n",
    "    # Generating the word embeddings for the processed sentence using the generate_word_embeddings function.\n",
    "    sent_vector = generate_word_embeddings(sent)\n",
    "    # Generating named entity recognition (NER) tags for the processed sentence using the generate_ner_tags function\n",
    "    ner_tag = generate_ner_tags(sent)\n",
    "    # Reshaping the extracted features to have the required shape for concatenation\n",
    "    x3 = np.reshape(np.array(sent_length), (-1, 1))\n",
    "    x1 = np.reshape(np.array(sent_vector), (-1, 1))\n",
    "    x2 = np.reshape(np.array(ner_tag), (-1, 1))\n",
    "    # Concatenating the features horizontally using the concatenate function\n",
    "    test = np.concatenate((x1, x2, x3),axis=0)\n",
    "    # Transposing the array to have the shape (1, n) where n is the total number of features\n",
    "    return test.T\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# The predictions_final function takes a sentence as input, applies pre-trained models to predict the categories for the input\n",
    "# sentence, and returns a list of the top three predicted categories. The function checks the predicted probability of the sentence \n",
    "# belonging to each category using a threshold, sorts the predicted categories by their probabilities in descending order, \n",
    "# and returns the top three predicted categories. If no categories were predicted, the function returns [\"Others\"].\n",
    "\n",
    "def predictions_final(sent):\n",
    "    # Initializing an empty list and dictionary to store the predicted categories and their probabilities\n",
    "    ls = []\n",
    "    mydict = {}\n",
    "    # Checking if the predicted probability of the sentence belonging to the 'Product' category is greater than the category threshold.\n",
    "    if models['Product'][0].predict_proba(sent)[:,1] > (models['Product'][1]):\n",
    "        mydict[\"Product\"] = (models['Product'][0].predict_proba(sent)[:,1])\n",
    "    # Checking if the predicted probability of the sentence belonging to the 'Cost' category is greater than the category threshold\n",
    "    if models['Cost'][0].predict_proba(sent)[:,1] > models['Cost'][1]:\n",
    "        mydict[\"Cost\"] = (models['Cost'][0].predict_proba(sent)[:,1])\n",
    "    # Checking if the predicted probability of the sentence belonging to the 'Sales' category is greater than the category threshold\n",
    "    if models['Sales'][0].predict_proba(sent)[:,1] > models['Sales'][1]:\n",
    "        mydict[\"Sales\"] = (models['Sales'][0].predict_proba(sent)[:,1])\n",
    "    # Checking if the predicted probability of the sentence belonging to the 'Customer Service' category is greater than the category threshold\n",
    "    if models['Customer Service'][0].predict_proba(sent)[:,1] > models['Customer Service'][1]:\n",
    "        mydict[\"Customer Service\"] = (models['Customer Service'][0].predict_proba(sent)[:,1])\n",
    "    # Checking if the predicted probability of the sentence belonging to the 'Training' category is greater than the category threshold\n",
    "    if models['Training'][0].predict_proba(sent)[:,1] > models['Training'][1]:\n",
    "        mydict[\"Training\"] = (models['Training'][0].predict_proba(sent)[:,1])\n",
    "    \n",
    "    # Sorting the dictionary by values (i.e. probabilities) in descending order and taking the top three predicted categories\n",
    "    mydict = dict(sorted(mydict.items(), key=lambda x: x[1], reverse = True)[:3])\n",
    "    # If no categories were predicted, return 'Others'\n",
    "    if len(mydict) == 0:\n",
    "        return([\"Others\"])\n",
    "    else:\n",
    "        # Appending the predicted categories to the list and returning it\n",
    "        for i,j in mydict.items():\n",
    "#             print(i, \" -> \", '{:.2%}'.format(j[0]))\n",
    "              ls.append(i)\n",
    "        return(ls)\n",
    "    \n",
    "# ------------------------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50a3be3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"NPS Data for ASU Team.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3a44d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing all the rows with nulls in the experience, and the primary category columns for the purpose of building this model.\n",
    "df.drop(df[df['How_can_we_improve_your_experience__c'].isnull()].index, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51a9952b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping rows from the DataFrame where the 'Primary_Category_1__c' column is not null\n",
    "df.drop(df[df['Primary_Category_1__c'].notnull()].index, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e022aeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open('model', 'rb')                                                # Opening Classifier in read-byte mode. \n",
    "models = pickle.load(pickle_in)                                                # Pickle load is used to load pickled data from a file-like object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e4eca16",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(nlp.get_pipe('ner').labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a815b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The values of the processed column are obtained by applying the testing function to the values of the How_can_we_improve_your_experience__c column of df. \n",
    "# The processed column will contain the transposed feature vectors for each row of the How_can_we_improve_your_experience__c column.\n",
    "df['processed'] = df['How_can_we_improve_your_experience__c'].apply(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9907e57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The predictions_final function is applied to each of these preprocessed sentences to obtain the predicted categories for each sentence. \\\n",
    "# The resulting prediction column in the DataFrame df contains the top three predicted categories for each sentence in the processed column.\n",
    "df['prediction'] = df['processed'].apply(predictions_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b23eb151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The lambda function takes a list x as input, extracts the first three elements (if they exist) and returns a Pandas Series with three values. \n",
    "# If x has fewer than three elements, the remaining values are set to None. The output of the lambda function is a DataFrame with three columns. \n",
    "df[['Primary_Category_1__c', 'Primary_Category_2__c', 'Primary_Category_3__c']] = df['prediction'].apply(lambda x: pd.Series([x[0], x[1] if len(x) > 1 else None, x[2] if len(x) > 2 else None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e593fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping prediction, and processed columns\n",
    "df.drop(['prediction','processed'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9bebf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving all the predictions\n",
    "df.to_csv('Categorized_data_{}.csv'.format(datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5fac7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
